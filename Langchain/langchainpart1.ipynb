{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7b359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef2b8f",
   "metadata": {},
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e682973",
   "metadata": {},
   "source": [
    "#### Model Name: qwen-qwq-32b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "277f6210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user introduced themselves as Prashant. I should greet them back and ask how I can assist. Keep it friendly and open-ended. Maybe mention their name to make it personal. Let me check if there's anything else I need to consider. No, that's straightforward. Just respond warmly and offer help.\n",
      "</think>\n",
      "\n",
      "Hello Prashant! Nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "response = model.invoke(\"Hi My name is Prashant\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d631c30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Enginner. Provide me answwer based on the query'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Enginner. Provide me answwer based on the query\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ccb7dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user is asking about LangGraph. Let me think about what I know. I remember that LangGraph was mentioned in some of the documents related to my training data. It was part of a system that involved large language models and graph-based processing.\n",
      "\n",
      "Wait, I should start by recalling if LangGraph is a specific tool or a concept. From what I gather, LangGraph might be a component within the LangChain framework? Or maybe it's a separate project. The user might be referring to a graph-based approach to handling information with LLMs.\n",
      "\n",
      "Hmm, I think LangGraph is part of a modular architecture designed to manage and optimize the use of large language models. The key idea is to break down complex tasks into smaller, manageable steps and represent them using a graph structure. Each node in the graph could be a sub-task, and edges define the dependencies between them. This allows for better organization of tasks and efficient reuse of intermediate results.\n",
      "\n",
      "Wait, but I need to be precise here. Let me check my mental notes. Oh right, LangGraph was introduced as part of a system to handle multi-step reasoning by structuring the problem into a graph of smaller language model calls. Each node represents a sub-task, and edges represent data flow between these tasks. This helps in managing the complexity of large tasks by decomposing them and using the graph to track dependencies and outputs.\n",
      "\n",
      "Also, there's something about reusability. Once a sub-task is processed, its result can be stored and reused in other parts of the graph, avoiding redundant computations. This is especially useful when dealing with iterative tasks or similar problems.\n",
      "\n",
      "So, to explain LangGraph, I should mention its purpose: managing complex tasks through a graph-based approach, enabling modular decomposition of tasks into sub-tasks, efficient data flow, and reusing results. It's part of optimizing LLM usage by breaking down problems into manageable chunks and tracking their execution.\n",
      "\n",
      "Wait, but is there more to it? Maybe it's part of a larger framework like LangChain? Or was LangGraph a standalone concept? Let me make sure. From my knowledge, LangChain is a framework for developers building LLM applications, and within that, LangGraph could be a specific tool or component that uses graph structures to organize and execute workflows involving LLMs.\n",
      "\n",
      "Alternatively, maybe LangGraph is an internal system used within my own architecture to handle context and task decomposition. For example, when I receive a query, I might break it down into steps, each represented as a node in a graph, and execute them in the correct order.\n",
      "\n",
      "I should also consider if there are any key features or benefits. The benefits would include better scalability, reusability of components, and the ability to handle complex tasks that require multiple steps. It also allows for parallel processing of independent sub-tasks, improving efficiency.\n",
      "\n",
      "Wait, but I need to be careful not to confuse it with similar concepts. For instance, there's Workflow Orchestration in general, but LangGraph is specifically tailored for LLM-based tasks. It uses the graph structure to represent the workflow's steps and their interdependencies.\n",
      "\n",
      "Putting this all together, the answer should define LangGraph as a graph-based system for organizing complex tasks into modular sub-tasks, managed via a graph structure to optimize execution and reuse of results. It's part of a larger system like LangChain, aiding in efficient and scalable use of LLMs for multi-step reasoning.\n",
      "\n",
      "I should also mention that LangGraph allows for tracking the execution of each node, handling dependencies, and managing the flow of data between different components. This ensures that each part of the task is processed optimally, and the overall task is completed efficiently without redundancy.\n",
      "\n",
      "Wait, maybe there's an example. Like, if a user asks for a document summary followed by sentiment analysis, LangGraph would structure that as two nodes: one for summarizing and another for analysis, with an edge from the first to the second since the summary is needed first.\n",
      "\n",
      "Also, it's important to note that this approach helps in debugging and modifying workflows since each node can be inspected or altered independently. That's a key benefit for developers using such systems.\n",
      "\n",
      "Okay, I think I have the main points. Now, structuring the answer clearly to explain what LangGraph is, its purpose, how it works with graphs, key features, and benefits. Make sure to highlight its role in managing complex LLM tasks efficiently.\n",
      "</think>\n",
      "\n",
      "**LangGraph** is a conceptual framework or system designed to manage and optimize the execution of complex tasks using large language models (LLMs). It employs a **graph-based approach** to decompose intricate problems into modular sub-tasks, enabling efficient, scalable, and reusable workflows. Here's a detailed breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Components & Functionality**:\n",
      "1. **Task Decomposition**:\n",
      "   - **Sub-tasks as Nodes**: Each node in the graph represents a discrete task or sub-task (e.g., \"summarize a document\" or \"extract key entities\").\n",
      "   - **Edges as Dependencies**: Edges between nodes define dependencies, ensuring tasks are executed in the correct order (e.g., \"sentiment analysis\" depends on \"summarization\" output).\n",
      "\n",
      "2. **Execution & Data Flow**:\n",
      "   - **Directed Acyclic Graph (DAG)**: Tasks are structured as a DAG to avoid circular dependencies, ensuring a logical workflow.\n",
      "   - **Intermediate Outputs**: Results from completed tasks (nodes) can be stored and reused across the graph, reducing redundant computations.\n",
      "\n",
      "3. **Modularity & Reusability**:\n",
      "   - **Reusable Components**: Nodes can be reused in different workflows, enabling developers to build modular pipelines.\n",
      "   - **Custom Workflows**: Users/developers can design tailored workflows by combining existing nodes to solve new problems.\n",
      "\n",
      "4. **Optimization & Efficiency**:\n",
      "   - **Parallel Processing**: Independent tasks (nodes without dependencies) can execute in parallel, speeding up processing.\n",
      "   - **Resource Management**: The graph structure helps in efficiently allocating computational resources to sub-tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### **Purpose & Benefits**:\n",
      "- **Handling Complex Tasks**: Breaks down multi-step reasoning or large-scale problems into manageable chunks, making them more computationally feasible for LLMs.\n",
      "- **Scalability**: Scales well with increasing complexity by organizing tasks logically and leveraging parallelism.\n",
      "- **Debugging & Maintenance**: Each node's output can be inspected independently, simplifying troubleshooting and updates.\n",
      "- **Reduced Redundancy**: Caches intermediate results to avoid recalculating outputs when dependencies remain unchanged.\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Workflow**:\n",
      "Suppose a user needs to **analyze a research paper**:\n",
      "1. **Node 1**: \"Extract key sections of the paper\" (e.g., introduction, methods, conclusions).\n",
      "2. **Node 2**: \"Summarize each section\" (depends on Node 1).\n",
      "3. **Node 3**: \"Generate a high-level summary of the entire paper\" (depends on Node 2).\n",
      "4. **Node 4**: \"Determine the paperâ€™s main research question\" (could depend on Node 3 or run in parallel if independent).\n",
      "\n",
      "The graph ensures tasks are executed in the correct order, with outputs from earlier nodes informing subsequent steps.\n",
      "\n",
      "---\n",
      "\n",
      "### **Where It Fits**:\n",
      "- **LangChain Integration**: Likely part of frameworks like [LangChain](https://github.com/langchain-ai/langchain), which provides tools for developers to build LLM-powered applications. LangGraph could be a component within LangChainâ€™s ecosystem for orchestrating workflows.\n",
      "- **Internal Use**: In systems like my own architecture, LangGraph principles might underpin how tasks like reasoning, context management, or multi-step inference are structured internally.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Advantages**:\n",
      "- **Flexibility**: Adaptable to diverse use cases (e.g., data analysis, content generation, decision-making).\n",
      "- **Transparency**: Developers can visualize and refine workflows by inspecting the graph structure.\n",
      "- **Cost Efficiency**: Reduces API call costs by minimizing redundant LLM interactions.\n",
      "\n",
      "---\n",
      "\n",
      "### **Use Cases**:\n",
      "- **Complex Reasoning**: Breaking down multi-step reasoning tasks (e.g., \"Compare two scientific papers and outline their methodological differences\").\n",
      "- **Pipeline Automation**: Automating workflows that require sequential or parallel processing (e.g., data ingestion â†’ analysis â†’ visualization).\n",
      "- **Dynamic Adaptation**: Adjusting workflows on-the-fly based on input changes (e.g., reusing cached results for similar queries).\n",
      "\n",
      "---\n",
      "\n",
      "### **Limitations**:\n",
      "- **Complexity Management**: Designing and maintaining large graphs can become challenging without proper tooling.\n",
      "- **Dependency Tracking**: Ensuring correct dependency ordering is critical to avoid errors (e.g., a sub-task failing to provide required inputs).\n",
      "\n",
      "---\n",
      "\n",
      "### **In Summary**:\n",
      "LangGraph is a **graph-based workflow orchestration tool** that enhances the utilization of LLMs by:\n",
      "1. Decomposing problems into modular sub-tasks.\n",
      "2. Managing dependencies and data flow between tasks.\n",
      "3. Optimizing resource usage and execution efficiency.\n",
      "4. Enabling reusable and scalable LLM-based applications.\n",
      "\n",
      "If you're working with LangChain or similar frameworks, understanding LangGraph principles can help streamline complex applications involving multiple LLM calls or iterative processes.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "response = chain.invoke({\"input\": \"What is LangGraph?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bba43",
   "metadata": {},
   "source": [
    "#### Model Name: gemma2-9b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdda92a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619b4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002A8307C0C80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002A831C07B30>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e071264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I'm familiar with LangSmith! It's a powerful and innovative open-source platform developed by the excellent folks at Cohere.  \n",
      "\n",
      "Here's a breakdown of what makes LangSmith stand out:\n",
      "\n",
      "**What is LangSmith?**\n",
      "\n",
      "LangSmith is essentially a collaborative space for building, training, and evaluating large language models (LLMs). Think of it like a comprehensive toolkit designed specifically for working with text-based AI.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** LangSmith is built with accessibility in mind. Even if you're not a seasoned developer, you can navigate the platform and experiment with LLMs relatively easily.\n",
      "* **Streamlined Workflow:** It simplifies the entire LLM development process, from data preparation to model training and fine-tuning.\n",
      "* **Collaborative Environment:** LangSmith fosters teamwork. Multiple users can work together on the same project, share models, and iterate on ideas.\n",
      "* **Model Library:** It offers a growing collection of pre-trained LLMs that you can leverage for your tasks. No need to start from scratch!\n",
      "* **Experiment Tracking:**  LangSmith helps you keep track of your experiments, allowing you to compare different model architectures, training parameters, and datasets.\n",
      "\n",
      "**Who Benefits from LangSmith?**\n",
      "\n",
      "* **Researchers:** It provides a platform to explore new LLM architectures and training techniques.\n",
      "* **Developers:** They can quickly build and deploy custom LLMs for specific applications.\n",
      "* **Educators:** LangSmith can be a valuable tool for teaching students about AI and natural language processing.\n",
      "* **Anyone interested in experimenting with LLMs:**  The platform's user-friendliness makes it accessible to a broad audience.\n",
      "\n",
      "**Where to Learn More:**\n",
      "\n",
      "Visit the official LangSmith website and GitHub repository for detailed documentation, tutorials, and community support:\n",
      "\n",
      "* **Website:** [https://www.langsmith.com/](https://www.langsmith.com/)\n",
      "* **GitHub:** [https://github.com/cohere-ai/langsmith](https://github.com/cohere-ai/langsmith)\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith â€“ I'm happy to delve deeper into specific aspects!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec86e2",
   "metadata": {},
   "source": [
    "## StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a825d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I can definitely tell you about Langsmith! \n",
      "\n",
      "Langsmith is an open-source platform developed by the Hugging Face team that simplifies the process of fine-tuning large language models (LLMs). \n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Simplifies Fine-tuning:** Langsmith provides a user-friendly interface and tools that make it easier for developers and researchers to fine-tune LLMs for specific tasks. This means you can adapt powerful pre-trained models to your own needs, like text summarization, question answering, or code generation.\n",
      "* **Offers Experiment Tracking:**  It includes features for tracking your fine-tuning experiments, making it easier to compare different models, hyperparameters, and datasets.\n",
      "\n",
      "* **Supports Multiple Frameworks:** Langsmith is designed to work with popular deep learning frameworks like TensorFlow and PyTorch, giving you flexibility in your development workflow.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accessibility:**  Langsmith lowers the barrier to entry for fine-tuning LLMs, making it accessible to a broader range of users, even those without extensive machine learning expertise.\n",
      "* **Efficiency:** The platform streamlines the fine-tuning process, saving you time and resources.\n",
      "* **Collaboration:** Being open-source, Langsmith encourages community contributions, which can lead to improvements and new features.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "You can find more information and get started with Langsmith on the Hugging Face website: [https://huggingface.co/blog/introducing-langsmith](https://huggingface.co/blog/introducing-langsmith)\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c1b1",
   "metadata": {},
   "source": [
    "## JsonOutputParser with PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ee4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI assistants.', 'features': ['Modular design allows for easy customization and extension.', 'Supports multiple AI models, including open-weights options.', 'Provides tools for training, evaluating, and deploying models.', 'Offers a user-friendly interface for developing and testing AI chatbots.', 'Community-driven development with contributions from developers and researchers.'], 'website': 'https://github.com/cohere-ai/smith'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser(indent=2)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user question \\n {format_instructions}\\n {input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea810ba",
   "metadata": {},
   "source": [
    "## JsonOutputParser with ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "825bc078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"LangSmith is an open-source tool developed by the Gemma team at Google DeepMind. It's designed to simplify the process of building and training large language models (LLMs). \\n\\n  Here are some key features of LangSmith:\\n\\n* **User-friendly interface:**  LangSmith provides a graphical interface that makes it easier to interact with the LLM development process, even for users without extensive coding experience.\\n* **Modular design:** It allows users to easily combine and customize different components of the LLM pipeline, such as tokenizers, trainers, and evaluators.\\n* **Support for multiple frameworks:** LangSmith can be used with popular deep learning frameworks like PyTorch and TensorFlow.\\n* **Open-source and accessible:** Being open-source, LangSmith is freely available for anyone to use, modify, and contribute to.\\n\\n  LangSmith aims to democratize access to LLM development by making it more accessible and user-friendly.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "output_parser = JsonOutputParser(indent=2)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide the response in json. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f037c",
   "metadata": {},
   "source": [
    "## XMLOutputParser with PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa5a6511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```xml\n",
      "<information>\n",
      "  <name>Langsmith</name>\n",
      "  <description>Langsmith is an open-weights AI assistant developed by Google DeepMind. It is designed to be a versatile tool for understanding and generating text, making it suitable for a wide range of applications.</description>\n",
      "  <features>\n",
      "    <feature>Text Generation</feature>\n",
      "    <feature>Text Understanding</feature>\n",
      "    <feature>Dialogue Systems</feature>\n",
      "  </features>\n",
      "</information>\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user question \\n {format_instructions}\\n {input}\\n \",\n",
    "    input_variables = [\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model \n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517ac12",
   "metadata": {},
   "source": [
    "## XMLOutputParser with ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e611454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response><answer>Langsmith is an open-source tool developed by the Gemma team at Google DeepMind. It's designed to streamline the process of fine-tuning large language models (LLMs) for specific tasks. \n",
      "\n",
      "Here are some key features of Langsmith:\n",
      "\n",
      "* **User-friendly Interface:** Langsmith offers a web-based interface that simplifies the fine-tuning process, making it accessible to a wider range of users.\n",
      "* **Fine-tuning Techniques:** It supports various fine-tuning techniques like prompt engineering, instruction tuning, and reinforcement learning.\n",
      "* **Model Zoo:** Langsmith provides a repository of pre-trained LLMs that can be easily fine-tuned for different applications.\n",
      "* **Community Driven:** As an open-source project, Langsmith benefits from the contributions and feedback of a vibrant community of developers and researchers.\n",
      "\n",
      "Overall, Langsmith aims to democratize access to LLM fine-tuning, empowering individuals and organizations to customize and deploy powerful AI models for their specific needs.</answer></response>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "chain = prompt | model \n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f465815",
   "metadata": {},
   "source": [
    "## JsonOutputParser without Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cbb3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user question.\\n {format_instructions}\\n {input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\"input\": joke_query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5f516",
   "metadata": {},
   "source": [
    "## JsonOutputParser with Pydantic + PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039e2a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0.7)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke, indent=2)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n {format_instructions}\\n {input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": joke_query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d6895",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66a96b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_name': 'Samsung Galaxy S24 Ultra', 'details': 'Flagship smartphone with a 6.8-inch Dynamic AMOLED 2X display, up to Snapdragon 8 Gen 3 processor, 200MP main camera, integrated S Pen stylus, and 5G connectivity.', 'price': 1200}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\",temperature=0.7)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class ProductDetails(BaseModel):\n",
    "    name: str = Field(description=\"Name of the product\")\n",
    "    detail: str = Field(description=\"Description of the product\")\n",
    "    price: int = Field(description=\"Tentative price in USD\")\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=ProductDetails, indent=2)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. When asked about any product, respond in JSON with product name, details, and a tentative price in USD (integer).\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Tell me about the Samsung Galaxy S24 Ultra?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
